{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from rpy2 import robjects\n",
    "from rpy2.robjects import r, pandas2ri\n",
    "pandas2ri.activate()\n",
    "r('library(MASS)')\n",
    "\n",
    "from dataloader import load_raw, create_datasets, create_dataloaders\n",
    "from measures import quantile_loss, compute_intermittent_indicators, rho_risk\n",
    "from models import ModelConfigBuilder\n",
    "\n",
    "from statsmodels.discrete.discrete_model import NegativeBinomial, Poisson\n",
    "from statsmodels.discrete.count_model import ZeroInflatedNegativeBinomialP, ZeroInflatedPoisson\n",
    "from statsmodels.tools.tools import add_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"carparts\"\n",
    "method = \"NBs_new\"\n",
    "\n",
    "data_raw, data_info = load_raw(dataset_name, datasets_folder_path=os.path.join(\"..\",\"data\"))\n",
    "adi, cv2 = compute_intermittent_indicators(data_raw, data_info['h'])\n",
    "datasets = create_datasets(data_raw, data_info)\n",
    "\n",
    "max_lag = {\n",
    "    'carparts':44,\n",
    "    'OnlineRetail':56,\n",
    "    'Auto':16,\n",
    "    'RAF':45,\n",
    "    'M5':50\n",
    "}\n",
    "\n",
    "lags = np.arange(1,max_lag[dataset_name]) if dataset_name != \"M5\" else [1,2,3,4,5,6,7,8,9,10,15,20,25,30,50,80,100, 150]\n",
    "lags = np.unique(np.sort(np.concatenate([lags, np.array([data_info['h']*data_info['w']])])))\n",
    "\n",
    "quantiles = np.array([0.5, 0.8, 0.9, 0.95, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factor = []\n",
    "for y in datasets['valid']:\n",
    "    y = np.array(y['target'])\n",
    "    demand_mask = np.where(y != 0)\n",
    "    scale_factor.append(np.mean(y[demand_mask]))\n",
    "    \n",
    "# scale_factor = [1]*len(scale_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = np.stack([ x['target'] for x in datasets['valid']])\n",
    "test_data = np.stack([ x['target'] for x in datasets['test']])\n",
    "\n",
    "import pickle as pkl\n",
    "pkl.dump(valid_data, open(\"cache_global/toR/valid_data.pkl\",\"wb\"))\n",
    "pkl.dump(test_data, open(\"cache_global/toR/test_data.pkl\",\"wb\"))\n",
    "pkl.dump(lags, open(\"cache_global/toR/lags.pkl\",\"wb\"))\n",
    "pkl.dump(np.array(scale_factor), open(\"cache_global/toR/scale_factor.pkl\",\"wb\"))\n",
    "pkl.dump(method, open(\"cache_global/toR/method.pkl\",\"wb\"))\n",
    "pkl.dump(dataset_name, open(\"cache_global/toR/dataset_name.pkl\",\"wb\"))\n",
    "pkl.dump(data_info['h'], open(\"cache_global/toR/h.pkl\",\"wb\"))\n",
    "\n",
    "\n",
    "import subprocess\n",
    "subprocess.run(['Rscript', 'discrete-regression.R'], check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for lag in lags:\n",
    "#     print(lag)\n",
    "\n",
    "#     model_folder_name = method+\"l\" + str(lag) + \"__\" + dataset_name\n",
    "#     model_folder_path = os.path.join('discrete-regression', model_folder_name)\n",
    "\n",
    "#     # each series is lag-embedded into a matrix at the given AR order and these matrices are stacked together to create on big matrix\n",
    "#     X = []\n",
    "#     Y = []\n",
    "#     Xtest = []\n",
    "#     Ytest = []\n",
    "#     for ts, sf in zip(datasets[\"valid\"], scale_factor):\n",
    "#         ts = np.round(np.array(ts['target']) / sf)\n",
    "#         for t in range(lag, len(datasets['valid'][0]['target'])):\n",
    "#             X.append(ts[t-lag:t])\n",
    "#             Y.append(ts[t])\n",
    "#         Xtest.append(ts[-lag:])\n",
    "#     for ts in datasets[\"test\"]:\n",
    "#         ts = np.array(ts['target'])\n",
    "#         Ytest.append(ts[-data_info['h']:])\n",
    "            \n",
    "#     X, Y = np.array(X), np.array(Y)\n",
    "#     max_value = X.max()\n",
    "#     Xtest, actuals = np.array(Xtest), np.array(Ytest)\n",
    "\n",
    "#     # a maximum of 1 mio samples for training\n",
    "#     filter = np.random.choice(X.shape[0], size=min(X.shape[0], 1_000_000), replace=False)\n",
    "#     Xsub = X[filter]\n",
    "#     Ysub = Y[filter]\n",
    "\n",
    "#     X_df = pd.DataFrame(Xsub, columns=[f'X{i}' for i in range(1, lag + 1)])\n",
    "#     Y_series = pd.Series(Ysub, name='Y')\n",
    "\n",
    "#     data = pd.concat([Y_series, X_df], axis=1)\n",
    "#     r_data = pandas2ri.py2rpy(data)\n",
    "#     r.assign('r_data', r_data)\n",
    "\n",
    "#     print('  training glm.nb')\n",
    "#     formula = 'Y ~ ' + ' + '.join(X_df.columns)\n",
    "#     r(f'model <- glm.nb(formula = \"{formula}\", data = r_data, control = glm.control(maxit = {100}))')\n",
    "\n",
    "#     print('  forecasting...')\n",
    "#     B = 200\n",
    "#     samples = np.empty(shape=(data_info['h'], B, Xtest.shape[0]))\n",
    "#     for h in tqdm(range(data_info['h'])):\n",
    "#         if h==0:         \n",
    "#             X_test_df = pd.DataFrame(Xtest, columns=[f'X{i}' for i in range(1, lag + 1)])\n",
    "#             r_test_data = pandas2ri.py2rpy(X_test_df)\n",
    "#             r.assign('r_test_data', r_test_data)\n",
    "#             r('predicted_means <- predict(model, newdata = r_test_data, type = \"response\")')\n",
    "#             predicted_means = r('predicted_means')  \n",
    "#             r.assign('predicted_means', predicted_means)\n",
    "#             r(f'samples <- sapply(predicted_means, function(mu) rnbinom({B}, size = model$theta, mu = mu))')\n",
    "#             s = r('samples')  \n",
    "#             s[s > max_value] = max_value\n",
    "#             samples[h] = s\n",
    "#         else:\n",
    "#             for i in range(B):\n",
    "#                 Xtest_ = np.concatenate([Xtest[:,h:lag], samples[max(h-lag,0):h, i].T], axis=1)\n",
    "#                 X_test_df = pd.DataFrame(Xtest_, columns=[f'X{i}' for i in range(1, lag + 1)])\n",
    "#                 r_test_data = pandas2ri.py2rpy(X_test_df)\n",
    "#                 r.assign('r_test_data', r_test_data)\n",
    "#                 r('predicted_means <- predict(model, newdata = r_test_data, type = \"response\")')\n",
    "#                 # predicted_means = r('predicted_means') \n",
    "#                 r(f'samples <- rnbinom(length(predicted_means), size=model$theta, mu=predicted_means)')\n",
    "#                 s = r('samples')\n",
    "#                 s[s > max_value] = max_value\n",
    "#                 samples[h,i] = s\n",
    "#                 if (np.isnan(s).sum() > 0 or np.isinf(s).sum()>0): assert False\n",
    "                \n",
    "#     quantile_forecasts = np.transpose(np.quantile(samples, quantiles, method=\"higher\", axis=1), (2,0,1))\n",
    "        \n",
    "#     if not os.path.exists(path=model_folder_path):\n",
    "#         os.makedirs(model_folder_path)\n",
    "#     if lag==1: np.save(os.path.join(model_folder_path,\"actuals.npy\"), actuals)\n",
    "#     np.save(os.path.join(model_folder_path,\"qforecasts.npy\"), quantile_forecasts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset=\"intermittent_and_lumpy\"\n",
    "if subset == \"intermittent\":\n",
    "    filter, filter_label = np.logical_and(adi >= 1.32, cv2 < .49), \"intermittent\"\n",
    "elif subset == \"intermittent_and_lumpy\":\n",
    "    filter, filter_label = adi >= 1.32, \"intermittent_and_lumpy\"\n",
    "elif subset == \"all\":\n",
    "    filter, filter_label = np.tile(True, adi.size), \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.empty(shape=(len(datasets['test']), len(datasets['valid'][0]['target']), quantiles.size))\n",
    "for i in range(len(datasets['test'])):\n",
    "    tmp[i, :] = np.round(np.quantile(datasets['valid'][i]['target'], q=quantiles))\n",
    "res_base_scale_tmp = []\n",
    "for i in range(len(datasets['test'])):\n",
    "    res_base_scale_tmp.append(quantile_loss(\n",
    "        np.array(datasets['valid'][i]['target']).reshape(1,-1), \n",
    "        tmp[i].reshape(1,tmp[i].shape[0],tmp[i].shape[1]), \n",
    "        quantiles, avg=False))\n",
    "res_base_scale = {}\n",
    "for q in ['QL50','QL80','QL90','QL95','QL99']:\n",
    "    res_base_scale[q] = np.mean(np.vstack([res_base_scale_tmp[i][q] for i in range(len(datasets['test']))]), axis=1)\n",
    "\n",
    "scale = True\n",
    "fscale = lambda x, q: x / res_base_scale[q][filter, np.newaxis] if scale else x\n",
    "\n",
    "path = os.path.join(\"discrete-regression\")\n",
    "names = [folder for folder in os.listdir(path) \n",
    "                  if os.path.isdir(os.path.join(path, folder))]\n",
    "names_sub = [x for x in names if (x.split('__')[1] == dataset_name) and (x.split('__')[0].split('l')[0] == method)]\n",
    "M = {}\n",
    "actuals = np.load(os.path.join(path, method+'l1__'+dataset_name, \"actuals.npy\"))[filter]\n",
    "for n in names_sub:\n",
    "    quantile_forecasts = np.load(os.path.join(path, n, \"qforecasts.npy\"))[filter]\n",
    "    M[int(n.split('l')[1].split('__')[0])] = quantile_loss(actuals, np.transpose(quantile_forecasts, (0,2,1)), quantiles, avg=False)\n",
    "M = {key: M[key] for key in sorted(M.keys())}\n",
    "\n",
    "for k in M.keys():\n",
    "    for l in M[k].keys():\n",
    "        M[k][l] = fscale(M[k][l], l)\n",
    "\n",
    "# Local methods\n",
    "baseline_path = os.path.join(os.path.expanduser(\"~/switchdrive\"), \"iTS\", \"trained_models_baselines\")\n",
    "baselines_name = [folder for folder in os.listdir(baseline_path) \n",
    "                  if os.path.isdir(os.path.join(baseline_path, folder)) and os.path.exists(os.path.join(baseline_path, folder, 'metrics.json'))]\n",
    "baselines_name_sub = [x for x in baselines_name if x.split('__')[1] == dataset_name]\n",
    "\n",
    "iETS_actuals = np.load(os.path.join(baseline_path, np.array(baselines_name_sub)[[\"iETS\" in x for x in baselines_name_sub]][0], \"actuals.npy\"))[filter]\n",
    "iETS_quantile_forecasts = np.load(os.path.join(baseline_path, np.array(baselines_name_sub)[[\"iETS\" in x for x in baselines_name_sub]][0], \"qforecasts.npy\"))[filter]\n",
    "iETS = quantile_loss(iETS_actuals, iETS_quantile_forecasts, quantiles, avg=False)\n",
    "EmpQ_actuals = np.load(os.path.join(baseline_path, np.array(baselines_name_sub)[[\"EmpQ\" in x for x in baselines_name_sub]][0], \"actuals.npy\"))[filter]\n",
    "EmpQ_quantile_forecasts = np.load(os.path.join(baseline_path, np.array(baselines_name_sub)[[\"EmpQ\" in x for x in baselines_name_sub]][0], \"qforecasts.npy\"))[filter]\n",
    "EmpQ = quantile_loss(EmpQ_actuals, EmpQ_quantile_forecasts, quantiles, avg=False)\n",
    "\n",
    "assert iETS.keys() == EmpQ.keys()\n",
    "for k in iETS.keys():\n",
    "    iETS[k] = fscale(iETS[k],k)\n",
    "    EmpQ[k] = fscale(EmpQ[k],k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "pkl.dump(M, open(os.path.join('cache_global', method+'_'+dataset_name+\".pkl\"), 'wb'))\n",
    "pkl.dump(iETS, open(os.path.join('cache_global', \"iETS\"+'_'+dataset_name+\".pkl\"), 'wb'))\n",
    "pkl.dump(EmpQ, open(os.path.join('cache_global', \"EmpQ\"+'_'+dataset_name+\".pkl\"), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = list(M[lags[0]].keys())\n",
    "fig, axs = plt.subplots(1,len(Q), figsize=(10,5), sharey=True)\n",
    "colors = ['#87CEFA','#00BFFF','#1E90FF','#4682B4','#191970']\n",
    "for ax, c, q_ in zip(axs, colors, Q):\n",
    "    tmp = [np.mean(M[l][q_]) for l in lags]\n",
    "    ax.hlines(y=np.mean(EmpQ[q_]), xmin=1, xmax=lags[-1], color=c, linestyle=':', label=\"EmpQ\")\n",
    "    ax.hlines(y=np.mean(iETS[q_]), xmin=1, xmax=lags[-1], color=c, linestyle='--', label=\"iETS\")\n",
    "    ax.plot(lags, tmp, color=c, label=\"glm.nb\")\n",
    "    ax.set_title(q_)\n",
    "axs[2].set_xlabel('LAG')\n",
    "axs[0].set_ylabel('QL')\n",
    "axs[-1].legend(loc=\"upper right\")\n",
    "plt.suptitle(dataset_name)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "its",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
