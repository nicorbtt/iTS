{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f7e1e88a93bc94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.scale import scale_factory\n",
    "\n",
    "from dataloader import load_raw, create_datasets, create_dataloaders\n",
    "from measures import quantile_loss, compute_intermittent_indicators, rho_risk\n",
    "\n",
    "from gluonts.torch.model.simple_feedforward import SimpleFeedForwardEstimator\n",
    "from gluonts.torch.distributions import NegativeBinomialOutput\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from gluonts.evaluation import make_evaluation_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c797edb06f5da95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"RAF\"\n",
    "method = \"DNNs_new\"\n",
    "\n",
    "data_raw, data_info = load_raw(dataset_name, datasets_folder_path=os.path.join(\"..\",\"data\"))\n",
    "adi, cv2 = compute_intermittent_indicators(data_raw, data_info['h'])\n",
    "datasets = create_datasets(data_raw, data_info)\n",
    "\n",
    "max_lag = {\n",
    "    'carparts':44,\n",
    "    'OnlineRetail':56,\n",
    "    'Auto':16,\n",
    "    'RAF':45,\n",
    "    'M5':50\n",
    "}\n",
    "max_value = data_raw.dropna().values.max()\n",
    "\n",
    "lags = np.arange(1,max_lag[dataset_name]) if dataset_name != \"M5\" else [1,2,3,4,5,6,7,8,9,10,15,20,25,30,50,80,100,150]\n",
    "lags = np.unique(np.sort(np.concatenate([lags, np.array([data_info['h']*data_info['w']])])))\n",
    "quantiles = np.array([0.5, 0.8, 0.9, 0.95, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fc2900ea36a1cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_entry(x, i, scale=True):\n",
    "    x.pop('feat_static_cat', None)\n",
    "    x.pop('feat_dynamic_real', None)\n",
    "    x.pop('item_id', None)\n",
    "    x['target'] = np.array(x['target'])\n",
    "    return x\n",
    "    \n",
    "ds_train = ListDataset([prepare_entry(x, i) for i, x in enumerate(datasets['train'])], freq=data_info['freq'])\n",
    "ds_valid = ListDataset([prepare_entry(x, i) for i, x in enumerate(datasets['valid'])], freq=data_info['freq'])\n",
    "ds_test = ListDataset([prepare_entry(x, i, scale=False) for i, x in enumerate(datasets['test'])], freq=data_info['freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f38588d548191ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lag in lags:\n",
    "    print(lag)\n",
    "    model_folder_name = method+\"l\" + str(lag) + \"__\" + dataset_name\n",
    "    model_folder_path = os.path.join('dnn-regression', model_folder_name)\n",
    "    \n",
    "    estimator = SimpleFeedForwardEstimator(\n",
    "        scale=True,\n",
    "        prediction_length=data_info[\"h\"],\n",
    "        context_length=lag,\n",
    "        hidden_dimensions= [32, 32, 32, 32, 32],\n",
    "        lr= 1e-4,\n",
    "        weight_decay= 1e-8,\n",
    "        distr_output=NegativeBinomialOutput(),\n",
    "        batch_norm=False,\n",
    "        batch_size=64,\n",
    "        num_batches_per_epoch=50,\n",
    "        trainer_kwargs={'accelerator':'cpu', 'max_epochs':1_000, 'enable_progress_bar': False, 'gradient_clip_val':1.0,\n",
    "                        'callbacks': [EarlyStopping(monitor=\"val_loss\", patience=20, verbose=True, mode=\"min\")]},\n",
    "    )\n",
    "    \n",
    "    predictor = estimator.train(training_data=ds_train, validation_data=ds_valid)\n",
    "    \n",
    "    forecast_it, ts_it = make_evaluation_predictions(dataset=ds_test, predictor=predictor, num_samples=400)\n",
    "    forecasts, tss = list(forecast_it), list(ts_it)\n",
    "    actuals = np.array([x[-data_info[\"h\"]:].values.reshape(-1) for x in tss])\n",
    "    quantile_forecasts = np.empty(shape=(len(datasets['test']), quantiles.size, data_info['h']))\n",
    "    for i, f in enumerate(forecasts):\n",
    "        for qi, q in enumerate(quantiles):\n",
    "                quantile_forecasts[i,qi,:] = np.round(f.quantile(q))\n",
    "            \n",
    "    quantile_forecasts[quantile_forecasts > max_value] = max_value\n",
    "    \n",
    "    if not os.path.exists(path=model_folder_path):\n",
    "        os.makedirs(model_folder_path)\n",
    "    if lag==1: np.save(os.path.join(model_folder_path,\"actuals.npy\"), actuals)\n",
    "    np.save(os.path.join(model_folder_path,\"qforecasts.npy\"), quantile_forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e23fc68b83825dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset=\"intermittent_and_lumpy\"\n",
    "if subset == \"intermittent\":\n",
    "    filter, filter_label = np.logical_and(adi >= 1.32, cv2 < .49), \"intermittent\"\n",
    "elif subset == \"intermittent_and_lumpy\":\n",
    "    filter, filter_label = adi >= 1.32, \"intermittent_and_lumpy\"\n",
    "elif subset == \"all\":\n",
    "    filter, filter_label = np.tile(True, adi.size), \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "439bc65343915d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.empty(shape=(len(datasets['test']), len(datasets['valid'][0]['target']), quantiles.size))\n",
    "for i in range(len(datasets['test'])):\n",
    "    tmp[i, :] = np.round(np.quantile(datasets['valid'][i]['target'], q=quantiles))\n",
    "res_base_scale_tmp = []\n",
    "for i in range(len(datasets['test'])):\n",
    "    res_base_scale_tmp.append(quantile_loss(\n",
    "        np.array(datasets['valid'][i]['target']).reshape(1,-1), \n",
    "        tmp[i].reshape(1,tmp[i].shape[0],tmp[i].shape[1]), \n",
    "        quantiles, avg=False))\n",
    "res_base_scale = {}\n",
    "for q in ['QL50','QL80','QL90','QL95','QL99']:\n",
    "    res_base_scale[q] = np.mean(np.vstack([res_base_scale_tmp[i][q] for i in range(len(datasets['test']))]), axis=1)\n",
    "\n",
    "scale = True\n",
    "fscale = lambda x, q: x / res_base_scale[q][filter, np.newaxis] if scale else x\n",
    "\n",
    "path = os.path.join(\"dnn-regression\")\n",
    "names = [folder for folder in os.listdir(path) \n",
    "                  if os.path.isdir(os.path.join(path, folder))]\n",
    "names_sub = [x for x in names if (x.split('__')[1] == dataset_name) and (x.split('__')[0].split('l')[0] == method)]\n",
    "M = {}\n",
    "actuals = np.load(os.path.join(path, method+'l1__'+dataset_name, \"actuals.npy\"))[filter]\n",
    "for n in names_sub:\n",
    "    quantile_forecasts = np.load(os.path.join(path, n, \"qforecasts.npy\"))[filter]\n",
    "    M[int(n.split('l')[1].split('__')[0])] = quantile_loss(actuals, np.transpose(quantile_forecasts, (0,2,1)), quantiles, avg=False)\n",
    "M = {key: M[key] for key in sorted(M.keys())}\n",
    "\n",
    "for k in M.keys():\n",
    "    for l in M[k].keys():\n",
    "        M[k][l] = fscale(M[k][l], l)\n",
    "\n",
    "# Local methods\n",
    "baseline_path = os.path.join(os.path.expanduser(\"~/switchdrive\"), \"iTS\", \"trained_models_baselines\")\n",
    "baselines_name = [folder for folder in os.listdir(baseline_path) \n",
    "                  if os.path.isdir(os.path.join(baseline_path, folder)) and os.path.exists(os.path.join(baseline_path, folder, 'metrics.json'))]\n",
    "baselines_name_sub = [x for x in baselines_name if x.split('__')[1] == dataset_name]\n",
    "\n",
    "iETS_actuals = np.load(os.path.join(baseline_path, np.array(baselines_name_sub)[[\"iETS\" in x for x in baselines_name_sub]][0], \"actuals.npy\"))[filter]\n",
    "iETS_quantile_forecasts = np.load(os.path.join(baseline_path, np.array(baselines_name_sub)[[\"iETS\" in x for x in baselines_name_sub]][0], \"qforecasts.npy\"))[filter]\n",
    "iETS = quantile_loss(iETS_actuals, iETS_quantile_forecasts, quantiles, avg=False)\n",
    "EmpQ_actuals = np.load(os.path.join(baseline_path, np.array(baselines_name_sub)[[\"EmpQ\" in x for x in baselines_name_sub]][0], \"actuals.npy\"))[filter]\n",
    "EmpQ_quantile_forecasts = np.load(os.path.join(baseline_path, np.array(baselines_name_sub)[[\"EmpQ\" in x for x in baselines_name_sub]][0], \"qforecasts.npy\"))[filter]\n",
    "EmpQ = quantile_loss(EmpQ_actuals, EmpQ_quantile_forecasts, quantiles, avg=False)\n",
    "Tweedie_quantile_forecasts = np.load(os.path.join(os.path.expanduser(\"~/switchdrive\"), \"iTS\", \"trained_models_baselines\", f\"TweedieGP/{dataset_name}/qforecasts.npy\"))[filter]\n",
    "Tweedie = quantile_loss(iETS_actuals, Tweedie_quantile_forecasts, quantiles, avg=False)\n",
    "\n",
    "assert iETS.keys() == EmpQ.keys() == Tweedie.keys()\n",
    "for k in iETS.keys():\n",
    "    iETS[k] = fscale(iETS[k],k)\n",
    "    EmpQ[k] = fscale(EmpQ[k],k)\n",
    "    Tweedie[k] = fscale(Tweedie[k],k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13cb9583f6a20dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "pkl.dump(M, open(os.path.join('cache_global', method+'_'+dataset_name+\".pkl\"), 'wb'))\n",
    "pkl.dump(iETS, open(os.path.join('cache_global', \"iETS\"+'_'+dataset_name+\".pkl\"), 'wb'))\n",
    "pkl.dump(EmpQ, open(os.path.join('cache_global', \"EmpQ\"+'_'+dataset_name+\".pkl\"), 'wb'))\n",
    "pkl.dump(Tweedie, open(os.path.join('cache_global', \"Tweedie\"+'_'+dataset_name+\".pkl\"), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf86f1ff53dbb15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.mean\n",
    "\n",
    "Q = list(M[lags[0]].keys())\n",
    "fig, axs = plt.subplots(1,len(Q), figsize=(10,5), sharey=True)\n",
    "colors = ['#87CEFA','#00BFFF','#1E90FF','#4682B4','#191970']\n",
    "for ax, c, q_ in zip(axs, colors, Q):\n",
    "    tmp = np.array([np.nanmean(M[l][q_]) for l in lags])\n",
    "    ax.hlines(y=f(EmpQ[q_]), xmin=1, xmax=lags[-1], color=c, linestyle=':', label=\"EmpQ\")\n",
    "    ax.hlines(y=f(iETS[q_]), xmin=1, xmax=lags[-1], color=c, linestyle='--', label=\"iETS\")\n",
    "    ax.hlines(y=f(Tweedie[q_]), xmin=1, xmax=lags[-1], color=\"magenta\", linestyle='--', label=\"TweedieGP\")\n",
    "    ax.plot(lags, tmp, color=c, label=\"DNN\")\n",
    "    ax.set_title(q_)\n",
    "axs[2].set_xlabel('LAG')\n",
    "axs[0].set_ylabel('QL')\n",
    "axs[0].legend(loc=\"upper left\")\n",
    "plt.suptitle(dataset_name)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "its",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
