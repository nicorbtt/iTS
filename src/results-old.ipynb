{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = os.path.join(os.path.expanduser(\"~/switchdrive\"), \"iTS\", \"trained_models\")\n",
    "experiments_name = [folder for folder in os.listdir(results_path) \n",
    "                    if os.path.isdir(os.path.join(results_path, folder)) and os.path.exists(os.path.join(results_path, folder, 'metrics.json'))]\n",
    "\n",
    "experiments_dict = {\n",
    "    'datetime':[],\n",
    "    'dataset':[],\n",
    "    'model':[],\n",
    "    'distribution_head':[],\n",
    "    'scaling':[],\n",
    "    'model_params':[],\n",
    "    'epochs':[],\n",
    "    'early_stop':[],\n",
    "    'actual':[],\n",
    "    'forecast':[],\n",
    "    'metrics':[]\n",
    "}\n",
    "\n",
    "baselines = ['iETS', 'ZeroForecast','EmpQ', 'ML_negbin', 'ML_zero-inf-pois', 'ML_tweedie', 'ML_poisson', 'ML_tweedie-fix']\n",
    "\n",
    "for exp in experiments_name:\n",
    "    skip = False\n",
    "    for b in baselines:\n",
    "        if b in exp: skip = True\n",
    "    exp_split = exp.split('__')\n",
    "    experiments_dict['dataset'].append( exp_split[1] )\n",
    "    experiments_dict['model'].append( exp_split[0] )\n",
    "    experiments_dict['actual'].append( os.path.join(results_path,exp,'actuals.npy') )\n",
    "    experiments_dict['forecast'].append( os.path.join(results_path,exp,'forecasts.npy') )\n",
    "    experiments_dict['metrics'].append( json.load(open(os.path.join(results_path,exp,'metrics.json'), \"r\")) )\n",
    "    if skip:\n",
    "        experiments_dict['distribution_head'].append( \"-\" )\n",
    "        experiments_dict['scaling'].append( '-' )\n",
    "        experiments_dict['epochs'].append( np.nan )\n",
    "        experiments_dict['early_stop'].append( np.nan )\n",
    "        experiments_dict['model_params'].append( np.nan )\n",
    "        experiments_dict['datetime'].append( exp_split[2] )\n",
    "        continue\n",
    "    exp_info = json.load(open(os.path.join(results_path,exp,'experiment.json'), \"r\"))\n",
    "    experiments_dict['distribution_head'].append( exp_info['distribution_head'] )\n",
    "    experiments_dict['scaling'].append( exp_info['scaling'] if exp_info['scaling'] else '-' )\n",
    "    experiments_dict['epochs'].append( exp_info['epoch'] )\n",
    "    experiments_dict['early_stop'].append( exp_info['early_stop'] )\n",
    "    experiments_dict['model_params'].append( json.load(open(os.path.join(results_path,exp,'model_params.json'), \"r\")) )\n",
    "    experiments_dict['datetime'].append( exp_split[4] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_df = pd.DataFrame(experiments_dict)\n",
    "experiments_df['datetime'] = pd.to_datetime(experiments_df['datetime'], format=\"%Y-%m-%d-%H-%M-%S-%f\")\n",
    "# -- \n",
    "experiments_df = experiments_df[(experiments_df.dataset == \"M5\") & ((experiments_df.model.isin([\"deepAR\"])) | experiments_df.model.isin(baselines)) &\n",
    "                                (~experiments_df.distribution_head.isin(['tweedie-priors']) )]\n",
    "experiments_df.sort_values('datetime', inplace=True)\n",
    "# --\n",
    "experiments_df.set_index(['datetime','dataset','model','distribution_head','scaling'], inplace=True)\n",
    "\n",
    "quantile_losses = pd.DataFrame([values['metrics']['rho_risk_nan']['intermittent_and_lumpy'] for _, values in experiments_df.iterrows()]).round(3).astype(float)\n",
    "quantile_losses.set_index(experiments_df.index, inplace=True)\n",
    "quantile_losses.sort_values(['dataset', 'scaling', 'distribution_head', 'datetime'], inplace=True)\n",
    "styled_quantile_losses = quantile_losses.iloc[:,2:].style.apply(lambda x: (x <= x.groupby('scaling').transform('min')).map({\n",
    "        True: 'background-color: yellow',\n",
    "        False: None,\n",
    "    })).apply(lambda col: ['background-color: green' if v == col.min() else '' for v in col])\n",
    "\n",
    "styled_quantile_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import load_raw\n",
    "from measures import compute_intermittent_indicators\n",
    "\n",
    "ts_classification = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'carparts'\n",
    "model = 'deepAR'\n",
    "scaling = 'mase'\n",
    "distribution_heads = ['negbin','poisson', 'tweedie', 'tweedie-fix', 'zero-inf-pois']\n",
    "subset = 'intermittent'\n",
    "quantiles = [0.25, 0.5, 0.8, 0.9, 0.95, 0.99]\n",
    "\n",
    "if dataset not in ts_classification.keys():\n",
    "    data_raw, _ = load_raw(dataset, os.path.join(  \"../data\"))\n",
    "    adi, cv2 = compute_intermittent_indicators(data_raw)\n",
    "    ts_classification[dataset] = {'intermittent' : np.logical_and(adi >= 1.32, cv2 < .49),\n",
    "                                  'intermittent_and_lumpy' : adi >= 1.32}\n",
    "\n",
    "\n",
    "#ranks(dataset, model, scaling, distribution_heads, subset)\n",
    "    \n",
    "\n",
    "scores_to_rank = {'q'+str(q) : {} for q in quantiles}\n",
    "rank = {}\n",
    "\n",
    "for experiments_name in os.listdir(results_path):\n",
    "    exp = experiments_name.split('__')\n",
    "    if exp[0] == model and exp[1] == dataset and exp[3] == scaling and exp[2] in distribution_heads:\n",
    "        forecasts = np.load(os.path.join(results_path, experiments_name, 'forecasts.npy'))\n",
    "        qforecasts = np.transpose(np.quantile(forecasts, quantiles, axis=1), (1,2,0))\n",
    "        actuals = np.load(os.path.join(results_path, experiments_name, 'actuals.npy'))\n",
    "        \n",
    "        for j in range(len(quantiles)):\n",
    "            scores_to_rank['q'+str(quantiles[j])][exp[2]] = 2 * np.mean(np.abs((qforecasts[:,:,j] - actuals) * ((actuals <= qforecasts[:,:,j]) - quantiles[j])), axis=1)\n",
    "        \n",
    "for q in quantiles:\n",
    "    quantile_to_rank = pd.DataFrame(scores_to_rank['q'+str(q)]).iloc[ts_classification[dataset][subset]]\n",
    "    quantile_to_rank = quantile_to_rank[sorted(quantile_to_rank.columns)]\n",
    "\n",
    "    rank['q'+str(q)] = np.mean(1 + np.argsort(np.argsort(quantile_to_rank.values, axis=1), axis = 1), axis=0)\n",
    "\n",
    "\n",
    "pd.DataFrame(rank, index = sorted(quantile_to_rank.columns)).style.background_gradient(low=2, high=3, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LaTeX formatting #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_map = {'deepAR':'DeepAR', 'iETS':'iETS MNN', 'ZeroForecast':'Dirac', 'transformer':'Transformer', 'EmpQ':'Empirical quantiles',\n",
    "'poisson':'Poisson', 'negbin':'Negative Binomial', 'tweedie':'Tweedie', 'tweedie-fix':'Tweedie (fixed dispersion)', 'zero-inf-pois':'Zero-inflated Poisson', '-':'/',\n",
    "'-':'/', 'mean':'Mean', 'mean-demand':'Mean demand', 'mase':'MASE'}\n",
    "\n",
    "experiments_df = pd.DataFrame(experiments_dict)\n",
    "experiments_df.set_index(['datetime','dataset','model','distribution_head','scaling'], inplace=True)\n",
    "\n",
    "df_intermittent = pd.DataFrame([values['metrics']['quantile_loss']['intermittent'] for _, values in experiments_df.iterrows()]).round(3).astype(float)\n",
    "df_intermittent_and_lumpy = pd.DataFrame([values['metrics']['quantile_loss']['intermittent_and_lumpy'] for _, values in experiments_df.iterrows()]).round(3).astype(float)\n",
    "\n",
    "ql = pd.concat((df_intermittent, df_intermittent_and_lumpy), axis=1, keys=['Intermittent', 'Intermittent and lumpy'])\n",
    "ql.set_index(experiments_df.index, inplace=True)\n",
    "ql.rename_axis(['Datetime', 'Dataset', 'Model','Likelihood', 'Scaling'], inplace=True)\n",
    "ql.reset_index(level = ('Model', 'Likelihood', 'Scaling'), inplace=True)\n",
    "ql.replace(replace_map, inplace=True)\n",
    "ql.set_index(['Model', 'Likelihood', 'Scaling'], append=True, inplace=True)\n",
    "ql.sort_values(['Dataset', 'Scaling', 'Likelihood', 'Datetime'], inplace=True)\n",
    "ql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_latex(ql_sliced):\n",
    "    latex = ql_sliced.style.apply(lambda x: (x <= x.groupby('Scaling').transform('min')).map({\n",
    "        True: 'cellcolor:{yellow}',\n",
    "        False: None,\n",
    "    })).apply(lambda col: ['cellcolor:{green}' if v == col.min() else '' for v in col]).to_latex()\n",
    "    latex_list = latex.splitlines()\n",
    "    latex_list[0] =  r'\\tiny ' + latex_list[0].replace('lll', 'lll|')\n",
    "    latex_list[1] = latex_list[1].replace(' & '.join(latex_list[1].split(' & ')[:3]), ' & '.join(latex_list[2].split(' & ')[:3]))\n",
    "    latex_list[2] = r'\\toprule'\n",
    "    latex_list = [r'\\begin{center}'] + latex_list + [r'\\end{center}']\n",
    "    latex_list = ['\\cmidrule{4-9}\\n' + line if line.startswith(' & Negative Binomial') else line for line in latex_list]      \n",
    "\n",
    "    latex = '\\n'.join(latex_list)\n",
    "    latex = latex.replace('000', '')\n",
    "    print(latex)\n",
    "\n",
    "\n",
    "ql_sliced = ql['Intermittent'].loc[(ql.index.get_level_values('Dataset') == 'RAF') & (ql.index.get_level_values('Model').isin(['DeepAR']))].droplevel(['Datetime', 'Dataset'])\n",
    "print_latex(ql_sliced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"Baselines\":[\"iETS MNN\", \"Empirical quantiles\", 'Dirac'], \"Transformer\":[\"Transformer\"], \"DeepAR\":[\"DeepAR\"]}\n",
    "\n",
    "\n",
    "for dataset in [ 'M5']:\n",
    "\n",
    "    for subset in ['Intermittent', 'Intermittent and lumpy']:\n",
    "\n",
    "        print(\"\\n\" + r\"\\newpage\" + \"\\n\" + r\"\\section{\\texttt{\" + dataset + \"}, \" + subset.lower() + \" time series}\")\n",
    "        \n",
    "        for k in models.keys():\n",
    "\n",
    "            print(\"\\n\\subsection{\" + k + \"}\")\n",
    "\n",
    "            ql_sliced = ql[subset].loc[(ql.index.get_level_values('Dataset') == dataset) & (ql.index.get_level_values('Model').isin(models[k]))].droplevel(['Datetime', 'Dataset'])\n",
    "\n",
    "            print_latex(ql_sliced)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this chunk of code when you want to add a metrics to past models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from dataloader import load_raw\n",
    "from measures import rho_risk_sample, rho_risk, compute_intermittent_indicators\n",
    "\n",
    "\n",
    "ds_dict = {}\n",
    "\n",
    "\n",
    "for folder in tqdm(os.listdir( os.path.join(os.path.expanduser(\"~/switchdrive\"), \"iTS\", \"trained_models\"))):\n",
    "\n",
    "\n",
    "    if os.path.isfile(os.path.join(os.path.expanduser(\"~/switchdrive\"), \"iTS\", \"trained_models\", folder, 'forecasts.npy')):\n",
    "\n",
    "        exp = json.load(open(os.path.join(os.path.expanduser(\"~/switchdrive\"), \"iTS\", \"trained_models\", folder, 'experiment.json')))\n",
    "        dataset = exp['dataset']\n",
    "    \n",
    "        if dataset not in ds_dict.keys():\n",
    "            data_raw, data_infos = load_raw(dataset, '../data/')\n",
    "            data_raw = data_raw.dropna()\n",
    "        \n",
    "            adi, cv2 = compute_intermittent_indicators(data_raw)\n",
    "            idx_intermittent = np.logical_and(adi >= 1.32, cv2 < .49)\n",
    "            idx_intermittent_and_lumpy = adi >= 1.32\n",
    "        \n",
    "            ds_dict[dataset] = {'idx_intermittent':idx_intermittent, 'idx_intermittent_and_lumpy':idx_intermittent_and_lumpy}\n",
    "    \n",
    "        idx_intermittent = ds_dict[dataset]['idx_intermittent']\n",
    "        idx_intermittent_and_lumpy = ds_dict[dataset]['idx_intermittent_and_lumpy']\n",
    "        \n",
    "        forecasts = np.load(os.path.join(os.path.expanduser(\"~/switchdrive\"), \"iTS\", \"trained_models\", folder, 'forecasts.npy'))\n",
    "        actuals = np.load(os.path.join(os.path.expanduser(\"~/switchdrive\"), \"iTS\", \"trained_models\", folder, 'actuals.npy'))\n",
    "\n",
    "        assert forecasts.shape[0] == actuals.shape[0]\n",
    "        assert forecasts.shape[2] == actuals.shape[1]\n",
    "\n",
    "        metrics = json.load(open(os.path.join(os.path.expanduser(\"~/switchdrive\"), \"iTS\", \"trained_models\", folder, 'metrics.json')))\n",
    "\n",
    "        metrics['rho_risk_nan'] = {'all' : rho_risk_sample(actuals, forecasts, zero_denom = np.nan),\n",
    "                                   'intermittent' : rho_risk_sample(actuals[idx_intermittent,:], forecasts[idx_intermittent,:,:], zero_denom = np.nan),\n",
    "                                   'intermittent_and_lumpy' : rho_risk_sample(actuals[idx_intermittent_and_lumpy,:], forecasts[idx_intermittent_and_lumpy,:,:], zero_denom = np.nan)}\n",
    "        metrics['rho_risk_1'] = {'all' : rho_risk_sample(actuals, forecasts, zero_denom = 1.),\n",
    "                                 'intermittent' : rho_risk_sample(actuals[idx_intermittent,:], forecasts[idx_intermittent,:,:], zero_denom = 1.),\n",
    "                                 'intermittent_and_lumpy' : rho_risk_sample(actuals[idx_intermittent_and_lumpy,:], forecasts[idx_intermittent_and_lumpy,:,:], zero_denom = 1.)}\n",
    "        json.dump(metrics, open(os.path.join(os.path.expanduser(\"~/switchdrive\"), \"iTS\", \"trained_models\", folder, 'metrics.json'), \"w\"))\n",
    "        \n",
    "    elif os.path.isfile(os.path.join(os.path.expanduser(\"~/switchdrive\"), \"iTS\", \"trained_models\", folder, 'qforecasts.npy')):\n",
    "\n",
    "        dataset = folder.split('__')[1]\n",
    "    \n",
    "        if dataset not in ds_dict.keys():\n",
    "            data_raw, data_infos = load_raw(dataset, '../data/')\n",
    "            data_raw = data_raw.dropna()\n",
    "        \n",
    "            adi, cv2 = compute_intermittent_indicators(data_raw)\n",
    "            idx_intermittent = np.logical_and(adi >= 1.32, cv2 < .49)\n",
    "            idx_intermittent_and_lumpy = adi >= 1.32\n",
    "        \n",
    "            ds_dict[dataset] = {'idx_intermittent':idx_intermittent, 'idx_intermittent_and_lumpy':idx_intermittent_and_lumpy}\n",
    "\n",
    "        idx_intermittent = ds_dict[dataset]['idx_intermittent']\n",
    "        idx_intermittent_and_lumpy = ds_dict[dataset]['idx_intermittent_and_lumpy']\n",
    "\n",
    "        qforecasts = np.load(os.path.join(os.path.expanduser(\"~/switchdrive\"), \"iTS\", \"trained_models\", folder, 'qforecasts.npy'))\n",
    "        actuals = np.load(os.path.join(os.path.expanduser(\"~/switchdrive\"), \"iTS\", \"trained_models\", folder, 'actuals.npy'))\n",
    "\n",
    "        assert qforecasts.shape[0] == actuals.shape[0]\n",
    "        assert qforecasts.shape[1] == actuals.shape[1]\n",
    "\n",
    "        metrics = json.load(open(os.path.join(os.path.expanduser(\"~/switchdrive\"), \"iTS\", \"trained_models\", folder, 'metrics.json')))\n",
    "                          \n",
    "        metrics['rho_risk_nan'] = {'all' : rho_risk(actuals, qforecasts, zero_denom = np.nan),\n",
    "                                   'intermittent' : rho_risk(actuals[idx_intermittent,:], qforecasts[idx_intermittent,:,:], zero_denom = np.nan),\n",
    "                                   'intermittent_and_lumpy' : rho_risk(actuals[idx_intermittent_and_lumpy,:], qforecasts[idx_intermittent_and_lumpy,:,:], zero_denom = np.nan)}\n",
    "        metrics['rho_risk_1'] = {'all' : rho_risk(actuals, qforecasts, zero_denom = 1.),\n",
    "                                 'intermittent' : rho_risk(actuals[idx_intermittent,:], qforecasts[idx_intermittent,:,:], zero_denom = 1.),\n",
    "                                 'intermittent_and_lumpy' : rho_risk(actuals[idx_intermittent_and_lumpy,:], qforecasts[idx_intermittent_and_lumpy,:,:], zero_denom = 1.)}\n",
    "\n",
    "        json.dump(metrics, open(os.path.join(os.path.expanduser(\"~/switchdrive\"), \"iTS\", \"trained_models\", folder, 'metrics.json'), \"w\"))\n",
    "        \n",
    "    elif folder == '.DS_Store':\n",
    "        continue\n",
    "    \n",
    "    else:\n",
    "        print('AIUTOOOOOOOO', folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this code if you want to check if there are missing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "trained_folders = os.listdir(os.path.join(os.path.expanduser(\"~/switchdrive\"), \"iTS\", \"trained_models\"))\n",
    "\n",
    "models = ['deepAR', 'transformer']\n",
    "datasets = ['M5', 'OnlineRetail', 'carparts', 'RAF', 'Auto']\n",
    "likelihoods = ['negbin', 'poisson', 'tweedie', 'tweedie-fix', 'zero-inf-pois']\n",
    "scalings = ['none', 'mase', 'mean', 'mean-demand']\n",
    "\n",
    "for el in itertools.product(models, datasets, likelihoods, scalings):\n",
    "    code = \"__\".join(el)\n",
    "    #if not any(folder.startswith(code) for folder in trained_folders):\n",
    "        #print(code)\n",
    "    count_folder = sum([1 if folder.startswith(code+'__') else 0 for folder in trained_folders])\n",
    "    if count_folder != 1:\n",
    "        print(code, count_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in ['M5', 'OnlineRetail', 'carparts', 'RAF', 'Auto']:\n",
    "    data_raw, data_infos = load_raw(dataset, '../data/')\n",
    "    data_raw = data_raw.dropna()\n",
    "    print(dataset, '\\t',np.sum(np.all(np.array(data_raw.iloc[:,-data_infos['h']:]) == 0, axis=1)), len(data_raw), '\\t',\n",
    "          str(100*np.sum(np.array(data_raw.iloc[:,-data_infos['h']:]) == 0)/(len(data_raw)*data_infos['h']))+'%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "its",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
