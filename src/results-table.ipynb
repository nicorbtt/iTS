{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results (table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataloader import load_raw, create_datasets\n",
    "from measures import compute_intermittent_indicators, quantile_loss_sample, quantile_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = os.path.join(os.path.expanduser(\"~/switchdrive\"), \"iTS\", \"trained_models\")\n",
    "experiments_name = [folder for folder in os.listdir(results_path) \n",
    "                    if os.path.isdir(os.path.join(results_path, folder)) and os.path.exists(os.path.join(results_path, folder, 'metrics.json'))]\n",
    "baseline_path = os.path.join(os.path.expanduser(\"~/switchdrive\"), \"iTS\", \"trained_models_baselines\")\n",
    "baselines_name = [folder for folder in os.listdir(baseline_path) \n",
    "                  if os.path.isdir(os.path.join(baseline_path, folder)) and os.path.exists(os.path.join(baseline_path, folder, 'metrics.json'))]\n",
    "\n",
    "def compute_table(subset, aggf=np.mean, scale=None):\n",
    "    tables = []\n",
    "    for dataset, model, N in zip([\"M5\",\"OnlineRetail\",\"carparts\",\"RAF\",\"Auto\"],[\"transformer\",\"deepAR\",\"deepAR\",\"deepAR\",\"deepAR\",\"deepAR\"],[5,10,10,10,10]):\n",
    "\n",
    "        scaling = \"mean-demand\"\n",
    "        distr = ['negbin','tweedie']\n",
    "\n",
    "        data_raw, data_info = load_raw(dataset_name=dataset, datasets_folder_path=os.path.join(\"..\",\"data\"))\n",
    "        datasets = create_datasets(data_raw, data_info)\n",
    "        adi, cv2 = compute_intermittent_indicators(data_raw, data_info['h'])\n",
    "        if subset == \"intermittent\":\n",
    "            filter, filter_label = np.logical_and(adi >= 1.32, cv2 < .49), \"intermittent\"\n",
    "        elif subset == \"intermittent_and_lumpy\":\n",
    "            filter, filter_label = adi >= 1.32, \"intermittent_and_lumpy\"\n",
    "        elif subset == \"all\":\n",
    "            filter, filter_label = np.tile(True, adi.size), \"all\"\n",
    "\n",
    "        experiments_name_sub = [x for x in experiments_name if x.split('__')[0] == model and x.split('__')[1] == dataset and x.split('__')[3] == scaling and x.split('__')[2] in distr]\n",
    "        sub = pd.DataFrame([x.split('__') for x in experiments_name_sub], columns=['model','dataset','distr','scaling','datetime'])\n",
    "\n",
    "        baselines_name_sub = [x for x in baselines_name if x.split('__')[1] == dataset]\n",
    "\n",
    "        quantiles = [0.5,0.8,0.9,0.95, 0.99]\n",
    "        tmp = np.empty(shape=(len(datasets['test']), len(datasets['valid'][0]['target']), len(quantiles)))\n",
    "        for i in range(len(datasets['test'])):\n",
    "            for j in range(len(quantiles)):\n",
    "                tmp[i, :] = np.round(np.quantile(datasets['valid'][i]['target'], q=quantiles))\n",
    "        res_base_scale_tmp = []\n",
    "        for i in range(len(datasets['test'])):\n",
    "            res_base_scale_tmp.append(quantile_loss(np.array(datasets['valid'][i]['target']).reshape(1,-1), tmp[i].reshape(1,tmp[i].shape[0],tmp[i].shape[1]), quantiles, avg=False))\n",
    "        res_base_scale = {}\n",
    "        for q in ['QL50','QL80','QL90','QL95','QL99']:\n",
    "            res_base_scale[q] = np.mean(np.vstack([res_base_scale_tmp[i][q] for i in range(len(datasets['test']))]), axis=1)[filter]\n",
    "\n",
    "        fscale = lambda x, q: x / res_base_scale[q][:, np.newaxis] if scale else x\n",
    "\n",
    "        res_base = {}\n",
    "        for exp in baselines_name_sub:\n",
    "            a = np.load(os.path.join(baseline_path, exp,\"actuals.npy\"))[filter]\n",
    "            qfc = np.load(os.path.join(baseline_path, exp,\"qforecasts.npy\"))[filter]\n",
    "            ql = quantile_loss(a, qfc, quantiles, avg=False)\n",
    "            res_base[exp.split('__')[0]] = {}\n",
    "            for q in ['QL50','QL80','QL90','QL95','QL99']:\n",
    "                res_base[exp.split('__')[0]][q] = aggf(fscale(ql[q], q))\n",
    "\n",
    "        res = {}\n",
    "        ql_full = {}\n",
    "        for d in distr:\n",
    "            sub_ = sub[sub.distr == d]\n",
    "            sub_paths = [\"__\".join(x.tolist()) for x in sub_.values]\n",
    "            assert len(sub_paths) == N\n",
    "\n",
    "            ql_all = {}\n",
    "            ql_full[d] = {}\n",
    "            for exp in sub_paths:\n",
    "                forecasts = np.load(os.path.join(results_path,exp,'forecasts.npy'))\n",
    "                actuals = np.load(os.path.join(results_path,exp,'actuals.npy'))\n",
    "                ql = quantile_loss_sample(actuals[filter,:], forecasts[filter,:,:], avg=False)\n",
    "                for q in ['QL50','QL80','QL90','QL95','QL99']:\n",
    "                    if not q in ql_all: ql_all[q] = []\n",
    "                    ql_all[q].append(aggf(fscale(ql[q], q)))\n",
    "                    if not q in ql_full[d]: ql_full[d][q] = []\n",
    "                    ql_full[d][q].append(ql[q])\n",
    "            ql_all_means = {key: np.mean(values) for key, values in ql_all.items()}\n",
    "            ql_all_std = {key: np.std(values) for key, values in ql_all.items()}\n",
    "\n",
    "            res[d] = {'mean':ql_all_means, 'std':ql_all_std}\n",
    "\n",
    "        tweedie_wins_mean = {}\n",
    "        tweedie_wins_std = {}\n",
    "        negbin_wins_mean = {}\n",
    "        negbin_wins_std = {}\n",
    "        for q in ['QL50','QL80','QL90','QL95','QL99']:\n",
    "            tweedie_wins_mean[q] = []\n",
    "            a = np.stack(ql_full['negbin'][q])\n",
    "            b = np.stack(ql_full['tweedie'][q])\n",
    "            a = np.mean(a, axis=2)\n",
    "            b = np.mean(b, axis=2)\n",
    "            tmp = [np.mean(b_ < a_)*100 for a_, b_ in zip(a, b)]\n",
    "            tweedie_wins_mean[q] = np.mean(tmp)\n",
    "            tweedie_wins_std[q] = np.std(tmp)\n",
    "            tmp = [np.mean(b_ > a_)*100 for a_, b_ in zip(a, b)]\n",
    "            negbin_wins_mean[q] = np.mean(tmp)\n",
    "            negbin_wins_std[q] = np.std(tmp)\n",
    "\n",
    "        def out(m, std=None, precision=3):\n",
    "            if std:\n",
    "                return [str(np.round(m[k],3))+'±'+str(np.round(std[k],3)) for k in m.keys()]\n",
    "            return [str(np.round(m[k],3)) for k in m.keys()]\n",
    "\n",
    "        print(dataset)\n",
    "        df = pd.DataFrame(data=[out(res_base['EmpQ']),\n",
    "                        out(res_base['iETS']),\n",
    "                        out(res['negbin']['mean'], res['negbin']['std']),\n",
    "                        out(res['tweedie']['mean'], res['tweedie']['std']),\n",
    "                        out(negbin_wins_mean, negbin_wins_std), out(tweedie_wins_mean, tweedie_wins_std)], \n",
    "                        index=['empQ','iETS',model+\"-negbin\",model+\"-tweedie\",\"negbin wins\",\"tweedie wins\"], \n",
    "                        columns=[\"QL50\",\"QL80\",\"QL90\",\"QL95\",\"QL99\"])\n",
    "        tables.append(df)\n",
    "        def highlight_min(s):\n",
    "            tmp = np.array([float(x.split('±')[0]) for x in s.values[:-2]])\n",
    "            is_min = tmp == np.min(tmp)\n",
    "            aaa = ['background-color: green' if v else '' for v in is_min]\n",
    "            tmp = np.array([float(x.split('±')[0]) for x in s.values[-2:]])\n",
    "            is_max = tmp == np.max(tmp)\n",
    "            return  aaa + ['background-color: gold; color:black' if v else 'background-color: khaki; color:black' for v in is_max]\n",
    "        display(df.style.apply(highlight_min, axis=0))\n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = compute_table(subset=\"intermittent_and_lumpy\", aggf=np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = compute_table(subset=\"intermittent\", aggf=np.mean, scale=\"EmpQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = compute_table(subset=\"intermittent_and_lumpy\", aggf=np.mean, scale=\"EmpQ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "its",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
