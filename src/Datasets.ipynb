{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datasets\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Retail ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "taken from `https://archive.ics.uci.edu/dataset/352/online+retail`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import from online repos\n",
    "\n",
    "#pip install ucimlrepo\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo  \n",
    " \n",
    "online_retail = fetch_ucirepo(id=352) \n",
    "  \n",
    "print('METADATA') \n",
    "print(online_retail.metadata) \n",
    "  \n",
    "print(\"VARIABLES\") \n",
    "print(online_retail.variables) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset exploration\n",
    "\n",
    "for k in online_retail.keys():\n",
    "    for j in online_retail[k].keys():\n",
    "        x= online_retail[k][j]\n",
    "        if hasattr(x, 'shape'):\n",
    "            print(k,j, x.shape)\n",
    "        else: \n",
    "            print(k,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep the relevant features\n",
    "\n",
    "df = online_retail['data']['features'][['Description', 'InvoiceDate', 'Quantity']]\n",
    "\n",
    "items = df.Description.unique()\n",
    "items = sorted([str(item) for item in items])\n",
    "\n",
    "df.loc[:,'InvoiceDate'] = pd.to_datetime(df.InvoiceDate).dt.date\n",
    "start, end = min(df.InvoiceDate), max(df.InvoiceDate)\n",
    "\n",
    "df = df.groupby(['Description', 'InvoiceDate'], group_keys=False)['Quantity'].sum()\n",
    "\n",
    "start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill the missing zeros\n",
    "\n",
    "items = items[:items.index('add stock to allocate online orders')]\n",
    "\n",
    "items_new = []\n",
    "data_dict = {}\n",
    "\n",
    "for item in items:\n",
    "    if any(df[item] < 0):\n",
    "        continue\n",
    "    ts = df[item].reindex(pd.date_range(start, end),fill_value=0)\n",
    "    data_dict[item] = {\n",
    "        'Values' : ts.values,\n",
    "        'Date' : ts.index\n",
    "    }\n",
    "    items_new.append(item)\n",
    "\n",
    "len(items_new), len(items), len(data_dict[items_new[0]]['Values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just make a plot\n",
    "\n",
    "item = list(data_dict.keys())[0]\n",
    "ts = data_dict[item]\n",
    "plt.plot(ts['Date'], ts['Values'])\n",
    "ts['Values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save it into the train?test format for datsets.data_loader\n",
    "\n",
    "path = \"/Users/stefano.damato/switchdrive/Private/PhD/data/OnlineRetail/\"\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    'start' : [str(data_dict[item]['Date'][0]) for item in items_new],\n",
    "    'target' : [list(data_dict[item]['Values']) for item in items_new],\n",
    "    'feat_stat_cat' : [[i] for i in range(len(items_new))],\n",
    "    'item_id' : items_new\n",
    "})\n",
    "\n",
    "df_train = pd.DataFrame({\n",
    "    'start' : [str(data_dict[item]['Date'][0]) for item in items_new],\n",
    "    'target' : [list(data_dict[item]['Values'])[:-28] for item in items_new],\n",
    "    'feat_stat_cat' : [[i] for i in range(len(items_new))],\n",
    "    'item_id' : items_new\n",
    "})\n",
    "\n",
    "df_train.to_json(path + 'train.json', orient='records')\n",
    "df_test.to_json(path + 'test.json', orient='records')\n",
    "\n",
    "pd.read_json(path + 'train.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save a .csv too\n",
    "\n",
    "pd.DataFrame(np.array([list(data_dict[item]['Values']) for item in items_new],\n",
    "                      dtype=np.int64)).to_csv(path + 'data.csv', index=False)\n",
    "\n",
    "pd.read_csv(path + 'data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.load_dataset(path, data_files={'train':'train.json', 'test':'test.json'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "taken from `https://github.com/canerturkmen/gluon-ts/tree/intermittent-datasets/datasets/intermittent_auto`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get .json from url\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "test_url = 'https://raw.githubusercontent.com/canerturkmen/gluon-ts/intermittent-datasets/datasets/intermittent_auto/test/data.json'\n",
    "train_url = 'https://raw.githubusercontent.com/canerturkmen/gluon-ts/intermittent-datasets/datasets/intermittent_auto/train/data.json'\n",
    "\n",
    "train = pd.read_json(train_url)\n",
    "test = pd.read_json(test_url)\n",
    "\n",
    "print(len(train.target[0]), len(test.target[0]))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the .csv train and test\n",
    "from datasets import Dataset\n",
    "\n",
    "path = \"/Users/stefano.damato/switchdrive/Private/PhD/data/Auto/\"\n",
    "\n",
    "test.to_json(path + \"test.json\", orient='records')\n",
    "train.to_json(path + \"train.json\", orient='records')\n",
    "\n",
    "pd.read_json(path +  'train.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.load_dataset(path, data_files={'train':'train.json', 'test':'test.json'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect it into a .csv too\n",
    "\n",
    "data = np.empty((len(test), 24))\n",
    "\n",
    "for i, ts in enumerate(test.target):\n",
    "    data[i,] = np.array(ts, dtype=np.int64)\n",
    "\n",
    "pd.DataFrame(data).to_csv(path + 'data.csv', index=False)\n",
    "\n",
    "pd.read_csv(path + 'data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAF ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "taken from `https://github.com/canerturkmen/gluon-ts/tree/intermittent-datasets/datasets/intermittent_raf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start importing the data\n",
    "\n",
    "train_url = 'https://raw.githubusercontent.com/canerturkmen/gluon-ts/intermittent-datasets/datasets/intermittent_raf/train/data.json'\n",
    "test_url = 'https://raw.githubusercontent.com/canerturkmen/gluon-ts/intermittent-datasets/datasets/intermittent_raf/test/data.json'\n",
    "\n",
    "#path = \"/Users/stefano.damato/switchdrive/Private/PhD/data/RAF/\"\n",
    "\n",
    "train = pd.read_json(train_url)\n",
    "test = pd.read_json(test_url)\n",
    "\n",
    "print(len(train.target[0]), len(test.target[0]))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the .csv train and test\n",
    "\n",
    "path = \"/Users/stefano.damato/switchdrive/Private/PhD/data/RAF/\"\n",
    "\n",
    "test.to_json(path + \"test.json\", orient='records')\n",
    "train.to_json(path + \"train.json\", orient='records')\n",
    "\n",
    "pd.read_json(path +  'train.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.load_dataset(path, data_files={'train':'train.json', 'test':'test.json'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect it into a .csv too\n",
    "\n",
    "data = np.empty((len(test), 84))\n",
    "\n",
    "for i, ts in enumerate(test.target):\n",
    "    data[i,] = np.array(ts, dtype=np.int64)\n",
    "\n",
    "pd.DataFrame(data).to_csv(path + 'data.csv', index=False)\n",
    "\n",
    "pd.read_csv(path + 'data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## carparts ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the data from the original .csv\n",
    "\n",
    "path = \"/Users/stefano.damato/switchdrive/Private/PhD/data/carparts/\"\n",
    "\n",
    "pd.read_csv(path + 'carparts.csv', sep=';', index_col=0).T.to_csv(path + 'data.csv', index=False)\n",
    "\n",
    "pd.read_csv(path + 'data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the .csv\n",
    "\n",
    "data = pd.read_csv(path + 'data.csv')\n",
    "\n",
    "start = datetime.date(1998, 1, 1)\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    'start' : [str(start) for i in range(len(data))],\n",
    "    'target' : [list(data.iloc[i]) for i in range(len(data))],\n",
    "    'feat_stat_cat' : [[i] for i in range(len(data))],\n",
    "    'item_id' : ['T'+str(i) for i in range(len(data))],\n",
    "})\n",
    "\n",
    "df_train = pd.DataFrame({\n",
    "    'start' : [str(start) for i in range(len(data))],\n",
    "    'target' : [list(data.iloc[i])[:-6] for i in range(len(data))],\n",
    "    'feat_stat_cat' : [[i] for i in range(len(data))],\n",
    "    'item_id' : ['T'+str(i) for i in range(len(data))]\n",
    "})\n",
    "\n",
    "df_test.to_json(path + 'test.json', orient='records')\n",
    "df_train.to_json(path  + 'train.json', orient ='records')\n",
    "\n",
    "pd.read_json(path  + 'test.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.load_dataset(path, data_files={'train':'train.json', 'test':'test.json'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syph ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the data from the original .csv\n",
    "\n",
    "path = \"/Users/stefano.damato/switchdrive/Private/PhD/data/Syph/\"\n",
    "\n",
    "pd.read_csv(path + 'syph.csv', sep=';', index_col=0).T.to_csv(path + 'data.csv', index=False)\n",
    "\n",
    "data = pd.read_csv(path + 'data.csv')\n",
    "data.iloc[:,:-12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the .json\n",
    "\n",
    "start = datetime.datetime(2007, 1, 1)\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    'start' : [str(start) for i in range(len(data))],\n",
    "    'target' : [list(data.iloc[i]) for i in range(len(data))],\n",
    "    'feat_stat_cat' : [[i] for i in range(len(data))],\n",
    "    'item_id' : ['T'+str(i) for i in range(len(data))]\n",
    "})\n",
    "\n",
    "df_train = pd.DataFrame({\n",
    "    'start' : [str(start) for i in range(len(data))],\n",
    "    'target' : [list(data.iloc[i])[:-12] for i in range(len(data))],\n",
    "    'feat_stat_cat' : [[i] for i in range(len(data))],\n",
    "    'item_id' : ['T'+str(i) for i in range(len(data))]\n",
    "})\n",
    "\n",
    "df_test.to_json(path + 'test.json', orient='records')\n",
    "df_train.to_json(path  + 'train.json', orient='records')\n",
    "\n",
    "pd.read_json(path + 'test.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.load_dataset(path =path, data_files={'train':'train.json', 'test':'test.json'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M5 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the data from the original .csv\n",
    "\n",
    "path = \"/Users/stefano.damato/switchdrive/Private/PhD/data/M5/\"\n",
    "\n",
    "pd.read_csv(path + 'daily_ts.csv', index_col=0).to_csv(path + 'data.csv', index=False)\n",
    "\n",
    "data = pd.read_csv(path + 'data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the .json\n",
    "\n",
    "start = datetime.datetime(2011, 1, 29)\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    'start' : [str(start) for i in range(len(data))],\n",
    "    'target' : [list(data.iloc[i]) for i in range(len(data))],\n",
    "    'feat_stat_cat' : [[i] for i in range(len(data))],\n",
    "    'item_id' : ['T'+str(i) for i in range(len(data))]\n",
    "})\n",
    "\n",
    "df_train = pd.DataFrame({\n",
    "    'start' : [str(start) for i in range(len(data))],\n",
    "    'target' : [list(data.iloc[i])[:-12] for i in range(len(data))],\n",
    "    'feat_stat_cat' : [[i] for i in range(len(data))],\n",
    "    'item_id' : ['T'+str(i) for i in range(len(data))]\n",
    "})\n",
    "\n",
    "df_test.to_json(path + 'test.json', orient='records')\n",
    "df_train.to_json(path  + 'train.json', orient='records')\n",
    "\n",
    "pd.read_json(path + 'test.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOME INFOS #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics_scores import ts_classification\n",
    "\n",
    "path = \"/Users/stefano.damato/switchdrive/Private/PhD/data/\"\n",
    "\n",
    "dataset_names = [\"OnlineRetail\", \"Auto\", 'RAF', 'carparts', 'syph', 'M5']\n",
    "\n",
    "\n",
    "for name in dataset_names:\n",
    "    ds = datasets.load_dataset(path+name+'/', data_files={'train':'train.json', 'test':'test.json'})\n",
    "    \n",
    "    data = np.array(pd.read_csv(path+name+'/data.csv').values)\n",
    "    nans = np.any(np.isnan(data), axis=1)\n",
    "    #if any(nans):\n",
    "        #data = data[~nans,:]\n",
    "    #adi = 1/np.mean(np.where(data > .5, 1, 0), axis=1)[np.std(np.where(data > .5, 1, 0), axis=1) != 0]\n",
    "    adi, cv2 = ts_classification(data)\n",
    "    print(name, \n",
    "          '\\nsize:', len(ds['train']), \n",
    "          '\\tstart:', ds['train'][0]['start'],\n",
    "          '\\ttrain length:', len(ds['train'][0]['target']), \n",
    "          '\\ttest length:', len(ds['test'][0]['target']),\n",
    "          '\\tseries w/ nans:', np.sum(nans), \n",
    "          '\\tseries w/ ADI >= 1.32:', np.sum(adi>=1.32),\n",
    "          '\\tseries w/ ADI >= 1.32 and CV2 < 0.49:', np.sum(np.logical_and(adi >= 1.32, cv2 < .49)), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(name, pred_length = None, idx=None):\n",
    "\n",
    "    assert name in [\"OnlineRetail\", \"Auto\", 'RAF', 'carparts', 'syph']\n",
    "    path = \"../data/\" + name + '/'\n",
    "\n",
    "    if pred_length is None and idx is None:\n",
    "        ds = datasets.load_dataset(path=path, data_files={'train':'train.json', 'test':'test.json'})\n",
    "        ds['freq'] = '1M'\n",
    "        return ds\n",
    "    \n",
    "    else:\n",
    "        test = pd.read_json(path + 'test.json', orient='records')\n",
    "        if idx is None: \n",
    "            idx = list(range(test.shape[0]))\n",
    "        test, train = test.iloc[idx,:], test.iloc[idx,:]\n",
    "        train.target = [ts[:-pred_length] for ts in train.target]\n",
    "        return datasets.DatasetDict({'train':datasets.Dataset.from_pandas(train),\n",
    "                                     'test':datasets.Dataset.from_pandas(test)})\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
